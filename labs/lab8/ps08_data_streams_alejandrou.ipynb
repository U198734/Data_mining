{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 08: Data streams\n",
    "\n",
    "In this session we will take a large corpus of documents and compute some statistics using data streams methods.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Alejandro Pastor Rubio</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">alejandro.pastor01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">The current date here</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file contain lines of dialogue of a set of movies from the [Movie Dialog Corpus](https://www.kaggle.com/datasets/Cornell-University/movie-dialog-corpus). We will use the file `movie_lines.tsv` which contains the text of the dialogue, about 3 million words in about 300,000 lines of dialogue.\n",
    "\n",
    "During this practice, **we will never load this file in memory.**\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "INPUT_FILE = \"movie_lines.tsv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `read_by_words` is a [generator](https://wiki.python.org/moin/Generators), that is, a function that behaves as an iterator. This is a common pattern used in stream processing, and in Python is implemented with the `yield` keyword, instead of `return`.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "# Producer in Python that reads a filename by words\n",
    "def read_by_words(filename, max_words=-1, report_every=-1):\n",
    "    \n",
    "    # Open the input file\n",
    "    with gzip.open(INPUT_FILE, \"rt\", encoding='utf8') as file:\n",
    "        \n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "    \n",
    "        # Regular expression to identify words having 3 letters or more and beginning with a-z\n",
    "        word_expr = re.compile('^[a-z]{2,}$', re.IGNORECASE)\n",
    "\n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "            \n",
    "            elements = line.split(\"\\t\")\n",
    "            \n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "                                        \n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "                \n",
    "            for word in nltk.word_tokenize(text):\n",
    "                          \n",
    "                if word_expr.match(word):\n",
    "                    counter += 1\n",
    "                    \n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print(\"- Read %d words so far\" % (counter))\n",
    "                        else:\n",
    "                            print(\"- Read %d/%d words so far\" % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a first pass over the data. Here we will read only the first 300K words. Try with a larger limit if your computer is fast, with a lower limit if your computer is slow. Find something that makes one pass take about 30 seconds and use it for development.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current word 'nothing'\n",
      "Current word 'decisions'\n",
      "Current word 'the'\n",
      "Current word 'gon'\n",
      "Current word 'when'\n",
      "Current word 'and'\n",
      "Current word 'want'\n",
      "Current word 'said'\n",
      "Current word 'yeah'\n",
      "Current word 'certainly'\n",
      "Current word 'he'\n",
      "Current word 'that'\n",
      "- Read 100000/300000 words so far\n",
      "Current word 'who'\n",
      "Current word 'drink'\n",
      "Current word 'want'\n",
      "Current word 'ca'\n",
      "Current word 'can'\n",
      "Current word 'how'\n",
      "Current word 'you'\n",
      "Current word 'it'\n",
      "Current word 'monsieur'\n",
      "- Read 200000/300000 words so far\n",
      "Current word 'was'\n",
      "Current word 'this'\n",
      "Current word 'hungry'\n",
      "Current word 'on'\n",
      "Current word 'letter'\n",
      "Current word 'you'\n",
      "Current word 'never'\n",
      "Current word 'to'\n",
      "Current word 'plants'\n",
      "Current word 'na'\n",
      "Current word 'for'\n",
      "Current word 'is'\n",
      "Current word 'allows'\n",
      "Current word 'arguments'\n",
      "Current word 'more'\n",
      "Current word 'to'\n",
      "- Read 300000/300000 words so far\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "# Iterate through the file\n",
    "for word in read_by_words(INPUT_FILE, max_words=300000, report_every=100000):\n",
    "    # Prints 1/10000 of words\n",
    "    if random.random() < 0.0001:\n",
    "        print(\"Current word '%s'\" % (word)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if above gives an error about 'punkt'\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of loading the entire dataset in main memory, we will use reservoir sampling to determine approximately the top-10 words.\n",
    "\n",
    "**Reservoir sampling**: In reservoir sampling, if we have a reservoir of size S:\n",
    "\n",
    "* We store the first S elements of the stream\n",
    "* When the n<sup>th</sup> element arrives (let's call it X<sub>n</sub>):\n",
    "   * With probability 1 - s/n, we ignore this element.\n",
    "   * With probability s/n, we:\n",
    "      * Discard a random element from the reservoir\n",
    "      * Add element X<sub>n</sub> to the reservoir (calling *add_to_reservoir*)\n",
    "      \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `add_reservoir(reservoir, item, max_size)` that adds an item to the reservoir, maintaining its size. If the reservoir is already of size *max_size*, a random item is selected and evicted *before* adding the item. It is important to evict an old item *before* adding the new item. Use the following skeleton:\n",
    "\n",
    "```python\n",
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    # YOUR CODE HERE\n",
    "    assert(len(reservoir) <= max_reservoir_size)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"add_reservoir\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    if len(reservoir) < max_reservoir_size:\n",
    "        reservoir.append(item)\n",
    "    else:\n",
    "        x = random.randint(0, len(reservoir)-1)\n",
    "        \n",
    "        reservoir[x] = item\n",
    "    \n",
    "    assert(len(reservoir) <= max_reservoir_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to iterate through the file using the reservoir sampling method seen in class. In this function you will decide, for every item, whether to call *add_to_reservoir* or to ignore the item.\n",
    "\n",
    "You can use the following skeleton:\n",
    "\n",
    "```python\n",
    "def reservoir_sampling(filename, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    \n",
    "    words_read = 0\n",
    "    \n",
    "    for word in read_by_words(filename, max_words=max_words, report_every=report_every):\n",
    "    \n",
    "            # YOUR CODE HERE\n",
    "\n",
    "    return (words_read, reservoir)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"reservoir_sampling\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(filename, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    \n",
    "    words_read = 0\n",
    "    \n",
    "    for word in read_by_words(filename, max_words=max_words, report_every=report_every):\n",
    "            words_read += 1\n",
    "            \n",
    "            #Añadimos los s primeros\n",
    "            if words_read < reservoir_size:\n",
    "                add_to_reservoir(reservoir, word, reservoir_size)\n",
    "            else:\n",
    "                prob = random.random()\n",
    "                #Con prob s/n descartamos uno existente y añadimos\n",
    "                if prob <= reservoir_size/words_read:\n",
    "                    add_to_reservoir(reservoir, word, reservoir_size)\n",
    "                \n",
    "                #Con prob 1 - s/n descartamos el actual\n",
    "                \n",
    "                \n",
    "                \n",
    "    return (words_read, reservoir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function using the following code:\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 100000/1000000 words so far\n",
      "- Read 200000/1000000 words so far\n",
      "- Read 300000/1000000 words so far\n",
      "- Read 400000/1000000 words so far\n",
      "- Read 500000/1000000 words so far\n",
      "- Read 600000/1000000 words so far\n",
      "- Read 700000/1000000 words so far\n",
      "- Read 800000/1000000 words so far\n",
      "- Read 900000/1000000 words so far\n",
      "- Read 1000000/1000000 words so far\n",
      "Number of items seen    : 1000028\n",
      "Number of items sampled : 1500\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=1000000, report_every=100000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reservoir contains repeated items. You can compute the absolute frequencies of the top 10 using the following code.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 you\n",
      "57 the\n",
      "51 to\n",
      "24 that\n",
      "24 it\n",
      "21 what\n",
      "21 of\n",
      "20 your\n",
      "18 we\n",
      "17 do\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:10]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to compute the 15 most frequent items in the reservoir and their relative frequencies, as percentages.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to print the top items and their relative frequencies</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative frequency: 5.0 % of the word: you\n",
      "Relative frequency: 3.8 % of the word: the\n",
      "Relative frequency: 3.4 % of the word: to\n",
      "Relative frequency: 1.6 % of the word: that\n",
      "Relative frequency: 1.6 % of the word: it\n",
      "Relative frequency: 1.4 % of the word: what\n",
      "Relative frequency: 1.4 % of the word: of\n",
      "Relative frequency: 1.33 % of the word: your\n",
      "Relative frequency: 1.2 % of the word: we\n",
      "Relative frequency: 1.13 % of the word: do\n",
      "Relative frequency: 1.07 % of the word: and\n",
      "Relative frequency: 1.0 % of the word: have\n",
      "Relative frequency: 0.93 % of the word: not\n",
      "Relative frequency: 0.93 % of the word: know\n",
      "Relative frequency: 0.93 % of the word: is\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:15]\n",
    "total_item = len(freq.items())\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"Relative frequency:\" ,round((absolute_frequency / len(reservoir)) * 100,2), \"%\",   \"of the word:\", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see an item C times in the reservoir, you can estimate the item appears *C x dataset_size / reservoir_size* times in the entire dataset (*dataset_size* is the size of the entire dataset). \n",
    "\n",
    "For various sizes of the reservoir, e.g., 50, 100, 500, ..., list the top-5 words and your estimate of their frequency in the entire dataset.\n",
    " \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Increase the max limit of words so that one pass takes no more than 5 minutes to be completed. Replace this cell with your code to try different reservoir sizes. In each case, print your estimate for the relative and absolute frequency of the words in the entire dataset.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 100000/10000000 words so far\n",
      "- Read 200000/10000000 words so far\n",
      "- Read 300000/10000000 words so far\n",
      "- Read 400000/10000000 words so far\n",
      "- Read 500000/10000000 words so far\n",
      "- Read 600000/10000000 words so far\n",
      "- Read 700000/10000000 words so far\n",
      "- Read 800000/10000000 words so far\n",
      "- Read 900000/10000000 words so far\n",
      "- Read 1000000/10000000 words so far\n",
      "- Read 1100000/10000000 words so far\n",
      "- Read 1200000/10000000 words so far\n",
      "- Read 1300000/10000000 words so far\n",
      "- Read 1400000/10000000 words so far\n",
      "- Read 1500000/10000000 words so far\n",
      "- Read 1600000/10000000 words so far\n",
      "- Read 1700000/10000000 words so far\n",
      "- Read 1800000/10000000 words so far\n",
      "- Read 1900000/10000000 words so far\n",
      "- Read 2000000/10000000 words so far\n",
      "- Read 2100000/10000000 words so far\n",
      "- Read 2200000/10000000 words so far\n",
      "- Read 2300000/10000000 words so far\n",
      "- Read 2400000/10000000 words so far\n",
      "- Read 2500000/10000000 words so far\n",
      "- Read 2600000/10000000 words so far\n",
      "- Read 2700000/10000000 words so far\n",
      "- Read 2800000/10000000 words so far\n",
      "- Read 2900000/10000000 words so far\n",
      "Number of items seen    : 2944884\n",
      "Number of items sampled : 50\n"
     ]
    }
   ],
   "source": [
    "# Size 50, max_words 10000000\n",
    "reservoir_size = 50\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=10000000, report_every=100000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 do\n",
      "Relative frequency: 6.0% of the word: do\n",
      "2 it\n",
      "Relative frequency: 4.0% of the word: it\n",
      "2 going\n",
      "Relative frequency: 4.0% of the word: going\n",
      "2 get\n",
      "Relative frequency: 4.0% of the word: get\n",
      "1 young\n",
      "Relative frequency: 2.0% of the word: young\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "total_item = len(freq.items())\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word) + \"\\n\" +\n",
    "      \"Relative frequency: \" + str(round((absolute_frequency / len(reservoir)) * 100, 2)) + \"%\" +\n",
    "      \" of the word: \" + word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 100000/10000000 words so far\n",
      "- Read 200000/10000000 words so far\n",
      "- Read 300000/10000000 words so far\n",
      "- Read 400000/10000000 words so far\n",
      "- Read 500000/10000000 words so far\n",
      "- Read 600000/10000000 words so far\n",
      "- Read 700000/10000000 words so far\n",
      "- Read 800000/10000000 words so far\n",
      "- Read 900000/10000000 words so far\n",
      "- Read 1000000/10000000 words so far\n",
      "- Read 1100000/10000000 words so far\n",
      "- Read 1200000/10000000 words so far\n",
      "- Read 1300000/10000000 words so far\n",
      "- Read 1400000/10000000 words so far\n",
      "- Read 1500000/10000000 words so far\n",
      "- Read 1600000/10000000 words so far\n",
      "- Read 1700000/10000000 words so far\n",
      "- Read 1800000/10000000 words so far\n",
      "- Read 1900000/10000000 words so far\n",
      "- Read 2000000/10000000 words so far\n",
      "- Read 2100000/10000000 words so far\n",
      "- Read 2200000/10000000 words so far\n",
      "- Read 2300000/10000000 words so far\n",
      "- Read 2400000/10000000 words so far\n",
      "- Read 2500000/10000000 words so far\n",
      "- Read 2600000/10000000 words so far\n",
      "- Read 2700000/10000000 words so far\n",
      "- Read 2800000/10000000 words so far\n",
      "- Read 2900000/10000000 words so far\n",
      "Number of items seen    : 2944884\n",
      "Number of items sampled : 2000\n"
     ]
    }
   ],
   "source": [
    "# Size 50, max_words 10000000\n",
    "reservoir_size = 2000\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=10000000, report_every=100000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 you\n",
      "Relative frequency: 4.65% of the word: you\n",
      "69 to\n",
      "Relative frequency: 3.45% of the word: to\n",
      "62 the\n",
      "Relative frequency: 3.1% of the word: the\n",
      "39 do\n",
      "Relative frequency: 1.95% of the word: do\n",
      "34 it\n",
      "Relative frequency: 1.7% of the word: it\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "total_item = len(freq.items())\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word) + \"\\n\" +\n",
    "      \"Relative frequency: \" + str(round((absolute_frequency / len(reservoir)) * 100, 2)) + \"%\" +\n",
    "      \" of the word: \" + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find by trial and error, and include in your report, the minimum reservoir size you need to have somewhat stable results (e.g., the same top-3 words in two consecutive runs of the algorithm).\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Remove the max limit of words and re-run. Replace this cell with a brief commentary indicating what reservoir size you would recommend to use, and your overall conclusions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 100000 words so far\n",
      "- Read 200000 words so far\n",
      "- Read 300000 words so far\n",
      "- Read 400000 words so far\n",
      "- Read 500000 words so far\n",
      "- Read 600000 words so far\n",
      "- Read 700000 words so far\n",
      "- Read 800000 words so far\n",
      "- Read 900000 words so far\n",
      "- Read 1000000 words so far\n",
      "- Read 1100000 words so far\n",
      "- Read 1200000 words so far\n",
      "- Read 1300000 words so far\n",
      "- Read 1400000 words so far\n",
      "- Read 1500000 words so far\n",
      "- Read 1600000 words so far\n",
      "- Read 1700000 words so far\n",
      "- Read 1800000 words so far\n",
      "- Read 1900000 words so far\n",
      "- Read 2000000 words so far\n",
      "- Read 2100000 words so far\n",
      "- Read 2200000 words so far\n",
      "- Read 2300000 words so far\n",
      "- Read 2400000 words so far\n",
      "- Read 2500000 words so far\n",
      "- Read 2600000 words so far\n",
      "- Read 2700000 words so far\n",
      "- Read 2800000 words so far\n",
      "- Read 2900000 words so far\n",
      "Number of items seen    : 2944884\n",
      "Number of items sampled : 1000\n"
     ]
    }
   ],
   "source": [
    "reservoir_size = 1000\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, report_every=100000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 you\n",
      "Relative frequency: 4.5% of the word: you\n",
      "44 the\n",
      "Relative frequency: 4.4% of the word: the\n",
      "29 to\n",
      "Relative frequency: 2.9% of the word: to\n",
      "25 it\n",
      "Relative frequency: 2.5% of the word: it\n",
      "22 and\n",
      "Relative frequency: 2.2% of the word: and\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "total_item = len(freq.items())\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word) + \"\\n\" +\n",
    "      \"Relative frequency: \" + str(round((absolute_frequency / len(reservoir)) * 100, 2)) + \"%\" +\n",
    "      \" of the word: \" + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will estimate the number of distinct words without creating a dictionary or hash table, but instead, we will use the Flajolet-Martin probabilistic counting method.\n",
    "\n",
    "**Flajolet-Martin probabilistic counting**:\n",
    "\n",
    "* For several passes\n",
    "   * Create hash funcion h\n",
    "   * For every element *u* in the stream:\n",
    "      * Compute hash value *h(u)*\n",
    "      * Let *r(u)* be the number of trailing zeroes in *h(u)*\n",
    "      * Maintain *R* as the maximum value of *r(u)* seen so far\n",
    "   * Add *2<sup>R</sup>* as an estimate for the number of distinct elements *u* seen\n",
    "* The final estimate is the average or the median of the estimates found in each pass\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to count trailing zeroes in the binary representation of a number.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to generate a random hash function. Note this generates a function, so you can do `hash_function = random_hash_function()` and then call `hash_function(x)` to compute the hash value of `x`. \n",
    "\n",
    "We want to make sure each hash is different, so we will create each hash function with a different [salt](https://en.wikipedia.org/wiki/Salt_(cryptography)), which is an additional input that we will take using a good random string generator from the [secrets](https://docs.python.org/3/library/secrets.html) library.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform *number_of_passes* passes over the file, reading the entire file on each pass (we don't use the reservoir in this part). In each pass, create a new hash function and use it to hash userids. Keep the maximum number of trailing zeroes seen in the hash value of a userid. \n",
    "\n",
    "```python\n",
    "number_of_passes = 5\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to perform the requested number of passes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate on pass 1: 16384 distinct words\n",
      "Estimate on pass 2: 16384 distinct words\n",
      "Estimate on pass 3: 16384 distinct words\n",
      "Estimate on pass 4: 32768 distinct words\n",
      "Estimate on pass 5: 32768 distinct words\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 5\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    h = random_hash_function()\n",
    "    R = -1\n",
    "    for word in read_by_words(INPUT_FILE,max_words=1000000):\n",
    "        h_u = h(word)\n",
    "        r_u = count_trailing_zeroes(h_u)\n",
    "        R = max(R,r_u)\n",
    "    estimate = 2**R\n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 22937.6\n",
      "* Median  of estimates: 16384.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the median of estimates obtained in 3 separate runs of your algorithm; each run should do 10 passes over the file. \n",
    "\n",
    "Increase the numbe of passes to 20 and perform 3 separate runs.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Remove the limit of max words, or set to a high number, but notice that you do no need to use more than one hour of computer processing time, and perform the 10 passes. Replace this cell with the results you obtained in each pass, and whether the average or the median seem more appropriate for this probabilistic counting.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate on pass 1: 8192 distinct words\n",
      "Estimate on pass 2: 131072 distinct words\n",
      "Estimate on pass 3: 65536 distinct words\n",
      "Estimate on pass 4: 65536 distinct words\n",
      "Estimate on pass 5: 262144 distinct words\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 10\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    h = random_hash_function()\n",
    "    R = -1\n",
    "    for word in read_by_words(INPUT_FILE):\n",
    "        h_u = h(word)\n",
    "        r_u = count_trailing_zeroes(h_u)\n",
    "        R = max(R,r_u)\n",
    "    estimate = 2**R\n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the previous cell, after 10 rounds of estimating the distinct words for all the text (We do not limit the words)\n",
    "We can see that the average of estamtes is around 35225 and the median is aroundd 16384.\n",
    "\n",
    "If we want to use this values to make some test or to arrive to a conlusion we may need to do more rounds, because this way we are removing the randomness and variance of each round.\n",
    "\n",
    "And if we look at the values of differnt rounds we can see that we have bigg variaty as: 8.000, 30.000, 100.000 so this reinforce the statement that we need to do more round to have an accurate estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELIVER (individually)\n",
    "\n",
    "Remember to read the section on \"delivering your code\" in the [course evaluation guidelines](https://github.com/chatox/data-mining-course/blob/master/upf/upf-evaluation.md).\n",
    "\n",
    "Deliver a zip file containing:\n",
    "\n",
    "* This notebook\n",
    "\n",
    "## Extra points available\n",
    "\n",
    "For more learning and extra points, notice that the number of **distinct** words in a corpus, as a function of the **total** number of words in the corpus, follows an empirical law known as [Heap's Law](https://en.wikipedia.org/wiki/Heaps%27_law).\n",
    "\n",
    "Repeat the probabilistic counting experiment for various values of `max_word` and plot the total number of words read versus the number of distinct words (remember to label axes). Check if it follows Heap's law.\n",
    "\n",
    "Please note that using probabilistic counting means a substantial amount of noise will be introduced and perhaps the Heap's law will not be clear in your plot.\n",
    "\n",
    "**Note:** if you go for the extra points, add ``<font size=\"+2\" color=\"blue\">Additional results: Heap's law</font>`` at the top of your notebook. \n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
